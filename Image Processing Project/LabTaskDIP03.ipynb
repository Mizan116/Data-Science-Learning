{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mqvfIsNSTuC",
        "outputId": "16971827-3e32-4d93-b321-d02a3fad4aa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat 'kaggle.json': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d shaunthesheep/microsoft-catsvsdogs-dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hfXoKG2Ssdx",
        "outputId": "b7333451-d450-441b-c47c-1a6e2d10dbd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/shaunthesheep/microsoft-catsvsdogs-dataset\n",
            "License(s): other\n",
            "Downloading microsoft-catsvsdogs-dataset.zip to /content\n",
            "100% 785M/788M [00:06<00:00, 143MB/s]\n",
            "100% 788M/788M [00:06<00:00, 121MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile('/content/microsoft-catsvsdogs-dataset.zip', 'r')\n",
        "zip_ref.extractall('/content')\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "p5GPyO4iSslI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imutils import paths\n",
        "import numpy as np\n",
        "import imutils\n",
        "import cv2\n",
        "import os\n",
        "import pickle as cPickle\n",
        "from sklearn.svm import SVC\n",
        "from google.colab.patches import cv2_imshow\n"
      ],
      "metadata": {
        "id": "8lwL7oCiSsnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from skimage.feature import graycomatrix, graycoprops\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "# Function to extract color histogram\n",
        "def extract_color_histogram(image):\n",
        "hist = cv2.calcHist([image], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256]) hist = cv2.normalize(hist, hist).flatten()\n",
        "return hist\n",
        "# Other Features\n",
        "# def extract_glcm_features(image):\n",
        "# # Convert the image to grayscale\n",
        "# gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "# # Compute GLCM\n",
        "# glcm = graycomatrix(gray, distances=[1], angles=[0], levels=256, symmetric=True, normed=True)\n",
        "# # Extract properties\n",
        "# contrast = graycoprops(glcm, 'contrast')[0, 0]\n",
        "# homogeneity = graycoprops(glcm, 'homogeneity')[0, 0]\n",
        "# energy = graycoprops(glcm, 'energy')[0, 0]\n",
        "# return [contrast, homogeneity, energy]\n",
        "# Load your data\n",
        "data = []\n",
        "labels = []\n",
        "imagePaths = list(paths.list_images('/content/dataset_train/train')) # Replace with actual image paths\n",
        "for (i, imagePath) in enumerate(imagePaths):\n",
        "image = cv2.imread(imagePath)\n",
        "label = imagePath.split(os.path.sep)[-1].split(\".\")[0]\n",
        "hist = extract_color_histogram(image)\n",
        "data.append(hist)\n",
        "labels.append(label)\n",
        "if i % 100 == 0:\n",
        "print(\"{}/{}\".format(i, len(imagePaths)))\n",
        "# Convert lists to numpy arrays\n",
        "data = np.array(data)\n",
        "labels = np.array(labels)\n"
      ],
      "metadata": {
        "id": "Gpb_vYnpSsqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "MaSPX_NkSssw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "data_scaled = scaler.fit_transform(data)\n"
      ],
      "metadata": {
        "id": "c39L-5h0W_8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply PCA to reduce to 2 dimensions for visualization\n",
        "pca = PCA(n_components=2)\n",
        "data_pca = pca.fit_transform(data_scaled)\n",
        "# Plotting the PCA results\n",
        "plt.figure(figsize=(8, 6))\n",
        "for label in np.unique(labels):\n",
        "    indices = np.where(labels == label)\n",
        "    plt.scatter(data_pca[indices, 0], data_pca[indices, 1], label=label, alpha=0.5)\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('2D PCA of Image Histogram Data')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fM9v2-TqW__c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA()\n",
        "data_pca = pca.fit_transform(data_scaled)\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
        "# Plotting the explained variance ratio\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, alpha=0.5, align='center', label='Individual explained variance')\n",
        "plt.step(range(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio, where='mid', label='Cumulative explained variance')\n",
        "plt.xlabel('Principal component index')\n",
        "plt.ylabel('Explained variance ratio')\n",
        "plt.title('Explained Variance Ratio by Principal Component')\n",
        "plt.legend(loc='best')\n",
        "plt.show()\n",
        "# Determine the number of components to retain based on a threshold (e.g., 95% of variance) threshold = 0.95\n",
        "num_components = np.argmax(cumulative_variance_ratio >= threshold) + 1\n",
        "print(f\"Number of components to retain 95% variance: {num_components}\")\n"
      ],
      "metadata": {
        "id": "DJCEA5vIXACM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "data_scaled = scaler.fit_transform(data)\n",
        "# Apply PCA to reduce to 2 dimensions for visualization\n",
        "pca = PCA(n_components=354)\n",
        "data_pca = pca.fit_transform(data_scaled)\n",
        "data_pca.shape\n",
        "\n"
      ],
      "metadata": {
        "id": "hxOIrCA2XAE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data_pca, labels, test_size=0.20, random_state=3)\n"
      ],
      "metadata": {
        "id": "DFESoNDCXAHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose the SVM kernel based on the PCA plot\n",
        "# Here, we'll start with linear, but you might choose 'rbf' based on the visualization svclassifier = SVC(kernel='rbf')\n",
        "svclassifier.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "SnbJ7i4ZXAKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = svclassifier.predict(X_test)\n",
        "print(classification_report(y_test, predictions))\n",
        " precision recall f1-score support\n",
        " cat 0.64 0.75 0.69 2562\n",
        " dog 0.68 0.55 0.61 2438\n",
        " accuracy 0.65 5000\n",
        " macro avg 0.66 0.65 0.65 5000\n",
        "weighted avg 0.66 0.65 0.65 5000\n"
      ],
      "metadata": {
        "id": "zO5yrH0rXkIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"/content/model.cPickle\", \"wb\")\n",
        "f.write(cPickle.dumps(svclassifier))\n",
        "f.close()\n"
      ],
      "metadata": {
        "id": "B3DE8tA5XkK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = cPickle.loads(open('/content/model.cPickle', \"rb\").read())"
      ],
      "metadata": {
        "id": "Q4pvDOJIXkNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose the SVM kernel based on the PCA plot\n",
        "# Here, we'll start with linear, but you might choose 'rbf' based on the visualization svclassifier = SVC(kernel= 'linear')\n",
        "svclassifier.fit(X_train, y_train)\n",
        "predictions = svclassifier.predict(X_test)\n",
        "print(classification_report(y_test, predictions))\n",
        " precision recall f1-score support\n",
        " cat 0.64 0.75 0.69 2562\n",
        " dog 0.68 0.55 0.61 2438\n",
        " accuracy 0.65 5000\n",
        " macro avg 0.66 0.65 0.65 5000\n",
        "weighted avg 0.66 0.65 0.65 5000\n"
      ],
      "metadata": {
        "id": "ebtxR8IXXkP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_imagePaths = list(paths.list_images('/content/dataset_test/test1'))\n",
        "def predict_single_img(img_path, model, scaler, pca):\n",
        "# Read the image\n",
        "singleImage = cv2.imread(img_path)\n",
        "print(singleImage.shape)\n",
        "# Extract color histogram\n",
        "hist = extract_color_histogram(singleImage).reshape(1, -1)\n",
        "print(hist.shape)\n",
        "# Display the image\n",
        "cv2_imshow(singleImage)\n",
        "# Scale the features\n",
        "hist_scaled = scaler.fit_transform(hist)\n",
        "# Apply PCA to reduce dimensionality to 354 features\n",
        "hist_pca = pca.transform(hist_scaled)\n",
        "# Predict using the model\n",
        "prediction = model.predict(hist_pca)\n",
        "print(prediction)\n",
        "predict_single_img(test_imagePaths[0], svclassifier, scaler, pca)\n"
      ],
      "metadata": {
        "id": "SQsS_ncvXtJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from skimage.feature import graycomatrix, graycoprops\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "# Function to extract color histogram (you should define this function as per your feature extraction method) def extract_color_histogram(image):\n",
        "hist = cv2.calcHist([image], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256]) hist = cv2.normalize(hist, hist).flatten()\n",
        "return hist\n",
        "# Other Features\n",
        "# def extract_glcm_features(image):\n",
        "# # Convert the image to grayscale\n",
        "# gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "# # Compute GLCM\n",
        "# glcm = graycomatrix(gray, distances=[1], angles=[0], levels=256, symmetric=True, normed=True)\n",
        "# # Extract properties\n",
        "# contrast = graycoprops(glcm, 'contrast')[0, 0]\n",
        "# homogeneity = graycoprops(glcm, 'homogeneity')[0, 0]\n",
        "# energy = graycoprops(glcm, 'energy')[0, 0]\n",
        "# return [contrast, homogeneity, energy]\n",
        "# Load your data\n",
        "data = []\n"
      ],
      "metadata": {
        "id": "RD3cKZuuXtLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = []\n",
        "imagePaths = list(paths.list_images('/content/dataset_train/train')) # Replace with actual image paths for(i, imagePath) in enumerate(imagePaths):\n",
        "image = cv2.imread(imagePath)\n",
        "label = imagePath.split(os.path.sep)[-1].split(\".\")[0]\n",
        "hist = extract_color_histogram(image)\n",
        "data.append(hist)\n",
        "if label==\"cat\":\n",
        "labels.append(0) #cat as 0\n",
        "else:\n",
        "labels.append(1) #dog as 1\n",
        "if i%100 == 0:\n",
        "print(\"{}/{}\".format(i, len(imagePaths)))\n",
        "# Convert lists to numpy arrays\n",
        "data = np.array(data)\n",
        "labels = np.array(labels)\n"
      ],
      "metadata": {
        "id": "ahzL7keDXtN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "data_scaled = scaler.fit_transform(data)\n",
        "# Apply PCA to reduce to 2 dimensions for visualization\n",
        "pca = PCA(n_components=354)\n",
        "data_pca = pca.fit_transform(data_scaled)\n"
      ],
      "metadata": {
        "id": "XZDEBuToXtQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(np.array(data_pca), labels, test_size = 0.20, random_state=3)"
      ],
      "metadata": {
        "id": "3FQZMfLvXtU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "3 Hidden Layers (1024, 256, 128 neurons) , 1 Output Layer with 1 neuron.\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "# Set random seed\n",
        "tf.random.set_seed(42)\n",
        "# Define the model\n",
        "model = tf.keras.Sequential([\n",
        "tf.keras.layers.Dense(1024, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "tf.keras.layers.Dense(256, activation='relu'),\n",
        "tf.keras.layers.Dense(128, activation='relu'),\n",
        "tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "# Compile the model\n",
        "model.compile(\n",
        "loss=tf.keras.losses.binary_crossentropy,\n",
        "optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "metrics=['accuracy']\n",
        ")\n",
        "# Display the model summary to show total number of parameters\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "YygzTFONX7T0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model and store the history\n",
        "history = model.fit(X_train, y_train, epochs=25, verbose=1)\n"
      ],
      "metadata": {
        "id": "S97c7apsX7WG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot loss and accuracy\n",
        "def plot_history(history):\n",
        "plt.figure(figsize=(12, 4))\n",
        "# Plot loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss over Epochs')\n",
        "plt.legend()\n",
        "# Plot accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy over Epochs')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plot_history(history)"
      ],
      "metadata": {
        "id": "oMC_ZxE9X7Yw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, predictions))\n",
        " precision recall f1-score support\n",
        " 0 0.62 0.85 0.71 2562\n",
        " 1 0.74 0.45 0.56 2438\n",
        " accuracy 0.65 5000\n",
        " macro avg 0.68 0.65 0.64 5000\n",
        "weighted avg 0.68 0.65 0.64 5000\n"
      ],
      "metadata": {
        "id": "JE3Q4u1JX7au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_single_img(img_path, model, scaler, pca):\n",
        "# Read the image\n",
        "singleImage = cv2.imread(img_path)\n",
        "print(singleImage.shape)\n",
        "# Extract color histogram\n",
        "hist = extract_color_histogram(singleImage).reshape(1, -1)\n",
        "print(hist.shape)\n",
        "# Display the image\n",
        "cv2_imshow(singleImage)\n",
        "# Scale the features\n",
        "hist_scaled = scaler.fit_transform(hist)\n",
        "# Apply PCA to reduce dimensionality to 354 features\n",
        "hist_pca = pca.transform(hist_scaled)\n",
        "# Predict using the model\n",
        "prediction = model.predict(hist_pca)\n",
        "if prediction>=0.5:\n",
        "print(\"dog\")\n",
        "else:\n",
        "print(\"cat\")\n",
        "predict_single_img(test_imagePaths[1000], model, scaler, pca)\n",
        "(358, 500, 3)\n",
        "(1, 512)\n"
      ],
      "metadata": {
        "id": "ZceKCaA7X7dB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "37GbppmzX7fE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eFyy01DCX7g9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oLJD7PHIX7jG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cb4BUzz3X7lR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}